{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. ELU\n",
    "*Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 1727s 10us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "# use the first 5000 images as validation set\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 32, 32, 3), (45000, 1), 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Explore lr\n",
    "*Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a reasonable learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = len(X) // batch_size * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr0 = 5e-4\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=lr0)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer = optimizer,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 10s 7ms/step - loss: nan - accuracy: 0.1066\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "rates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 0.05 (50e-3) train loss shoots up. 10 times smaller than that is 0.005 (5e-3). ---> This lr doesnt work well at all.\n",
    "Use what the author used instead. How did he find this rate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer = optimizer,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_index = run_index+1 if run_index else 1\n",
    "run_logdir = os.path.join(os.curdir, 'my_cifar10_logs', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# checkpoint_cb = keras.callbacks.ModelCheckpoint('my_models/cifar10.h5', save_best_only=True)\n",
    "# callbacks = [checkpoint_cb, earlystop_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1/1407 [..............................] - ETA: 0s - loss: 1.8068 - accuracy: 0.3125WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0335s). Check your callbacks.\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.8177 - accuracy: 0.3405 - val_loss: 1.7975 - val_accuracy: 0.3436\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7642 - accuracy: 0.3611 - val_loss: 1.7822 - val_accuracy: 0.3498\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.7181 - accuracy: 0.3780 - val_loss: 1.7215 - val_accuracy: 0.3824\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.6795 - accuracy: 0.3921 - val_loss: 1.7674 - val_accuracy: 0.3760\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.6503 - accuracy: 0.4043 - val_loss: 1.7025 - val_accuracy: 0.3838\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.6228 - accuracy: 0.4136 - val_loss: 1.6487 - val_accuracy: 0.4106\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5942 - accuracy: 0.4252 - val_loss: 1.6493 - val_accuracy: 0.4042\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5730 - accuracy: 0.4343 - val_loss: 1.6118 - val_accuracy: 0.4170\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5528 - accuracy: 0.4412 - val_loss: 1.6356 - val_accuracy: 0.4056\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.5364 - accuracy: 0.4472 - val_loss: 1.6183 - val_accuracy: 0.4148\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.5192 - accuracy: 0.4543 - val_loss: 1.6454 - val_accuracy: 0.4142\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.5002 - accuracy: 0.4620 - val_loss: 1.6074 - val_accuracy: 0.4236\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4849 - accuracy: 0.4638 - val_loss: 1.5949 - val_accuracy: 0.4262\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4748 - accuracy: 0.4710 - val_loss: 1.5560 - val_accuracy: 0.4402\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.4596 - accuracy: 0.4772 - val_loss: 1.5684 - val_accuracy: 0.4354\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4419 - accuracy: 0.4842 - val_loss: 1.5756 - val_accuracy: 0.4462\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4348 - accuracy: 0.4847 - val_loss: 1.5648 - val_accuracy: 0.4404\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4204 - accuracy: 0.4895 - val_loss: 1.5508 - val_accuracy: 0.4542\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4132 - accuracy: 0.4898 - val_loss: 1.5624 - val_accuracy: 0.4386\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4011 - accuracy: 0.4972 - val_loss: 1.5414 - val_accuracy: 0.4450\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3915 - accuracy: 0.5000 - val_loss: 1.5529 - val_accuracy: 0.4548\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3779 - accuracy: 0.5053 - val_loss: 1.5278 - val_accuracy: 0.4566\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.3673 - accuracy: 0.5078 - val_loss: 1.5481 - val_accuracy: 0.4508\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3588 - accuracy: 0.5121 - val_loss: 1.5524 - val_accuracy: 0.4534\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3482 - accuracy: 0.5163 - val_loss: 1.5217 - val_accuracy: 0.4626\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3373 - accuracy: 0.5193 - val_loss: 1.5396 - val_accuracy: 0.4530\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3278 - accuracy: 0.5213 - val_loss: 1.5760 - val_accuracy: 0.4438\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3218 - accuracy: 0.5240 - val_loss: 1.5284 - val_accuracy: 0.4562\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3129 - accuracy: 0.5288 - val_loss: 1.5184 - val_accuracy: 0.4632\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2992 - accuracy: 0.5336 - val_loss: 1.5942 - val_accuracy: 0.4480\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2955 - accuracy: 0.5357 - val_loss: 1.5389 - val_accuracy: 0.4628\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2849 - accuracy: 0.5384 - val_loss: 1.5503 - val_accuracy: 0.4646\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2787 - accuracy: 0.5402 - val_loss: 1.5534 - val_accuracy: 0.4638\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2730 - accuracy: 0.5444 - val_loss: 1.5360 - val_accuracy: 0.4610\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2593 - accuracy: 0.5482 - val_loss: 1.5563 - val_accuracy: 0.4578\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.2550 - accuracy: 0.5491 - val_loss: 1.5785 - val_accuracy: 0.4562\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 1.2449 - accuracy: 0.5525 - val_loss: 1.5598 - val_accuracy: 0.4586\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2381 - accuracy: 0.5556 - val_loss: 1.5260 - val_accuracy: 0.4672\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2271 - accuracy: 0.5584 - val_loss: 1.5464 - val_accuracy: 0.4670\n"
     ]
    }
   ],
   "source": [
    "n_epochs, batch_size = 100, 32\n",
    "history = model.fit(X_train, y_train, epochs = n_epochs,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks = [earlystop_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5184 - accuracy: 0.4632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5183756351470947, 0.46320000290870667]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = keras.models.load_model(\"my_models/cifar10.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save and load model leads to different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5184 - accuracy: 0.1104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5183756351470947, 0.1103999987244606]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.models.save_model(model, \"my_models/cifar10.h5\")\n",
    "model_loaded = keras.models.load_model(\"my_models/cifar10.h5\")\n",
    "model_loaded.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5184 - accuracy: 0.1104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5183756351470947, 0.1103999987244606]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = keras.models.load_model(\"my_models/cifar10.h5\")\n",
    "model_loaded.load_weights(\"my_models/cifar10.h5\")\n",
    "model_loaded.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_w = model_loaded.get_weights() \n",
    "original_w = model.get_weights()\n",
    "assert len(loaded_w) == len(original_w)\n",
    "for idx in range(len(original_w)):\n",
    "    assert np.array_equal(loaded_w[idx], original_w[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No assert errors. But why does the model evaluation return different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5184 - accuracy: 0.4632\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 1.5184 - accuracy: 0.1104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5183756351470947, 0.1103999987244606]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)\n",
    "model_loaded.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(model.predict_proba(X_valid), model_loaded.predict_proba(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Validation Accuracy 0.4632\n",
      "Loaded Model Validation Accuracy 0.4632\n"
     ]
    }
   ],
   "source": [
    "print('Original Model Validation Accuracy', accuracy_score(y_valid, np.argmax(model.predict_proba(X_valid),axis=1).reshape(-1, 1)))\n",
    "print('Loaded Model Validation Accuracy', accuracy_score(y_valid, np.argmax(model_loaded.predict_proba(X_valid),axis=1).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reloaded model returns different results/predictions when using loaded_model.evaluate(), even though the weights are correct and the same as the original model. Using model.predict_proba will give the correct evaluation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Batch Normalization\n",
    "*Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try out:\n",
    "- two BN models: 1. BN after activiation 2. BN before activation\n",
    "- try out several learning rates, compare their validation loss after 20 epochs & pick the best one\n",
    "- rename run directories to run_bn_* and the model file name to cifar10_bn_[bf|af]_model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 2.5833 - accuracy: 0.1372 - val_loss: 2.3243 - val_accuracy: 0.1854\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 2.2340 - accuracy: 0.2030 - val_loss: 2.1113 - val_accuracy: 0.2442\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 2.0992 - accuracy: 0.2450 - val_loss: 1.9924 - val_accuracy: 0.2856\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 2.0138 - accuracy: 0.2721 - val_loss: 1.9116 - val_accuracy: 0.3146\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.9473 - accuracy: 0.2986 - val_loss: 1.8598 - val_accuracy: 0.3256\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.8995 - accuracy: 0.3155 - val_loss: 1.8181 - val_accuracy: 0.3414\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.8634 - accuracy: 0.3311 - val_loss: 1.7774 - val_accuracy: 0.3538\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.8352 - accuracy: 0.3398 - val_loss: 1.7587 - val_accuracy: 0.3624\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.8058 - accuracy: 0.3534 - val_loss: 1.7329 - val_accuracy: 0.3718\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7827 - accuracy: 0.3628 - val_loss: 1.7123 - val_accuracy: 0.3788\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7652 - accuracy: 0.3706 - val_loss: 1.6928 - val_accuracy: 0.3924\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7445 - accuracy: 0.3773 - val_loss: 1.6757 - val_accuracy: 0.3936\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 1.7235 - accuracy: 0.3850 - val_loss: 1.6565 - val_accuracy: 0.4056\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.7129 - accuracy: 0.3893 - val_loss: 1.6412 - val_accuracy: 0.4160\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6956 - accuracy: 0.3942 - val_loss: 1.6281 - val_accuracy: 0.4132\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6853 - accuracy: 0.4013 - val_loss: 1.6173 - val_accuracy: 0.4238\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6693 - accuracy: 0.4042 - val_loss: 1.6054 - val_accuracy: 0.4262\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6552 - accuracy: 0.4096 - val_loss: 1.5965 - val_accuracy: 0.4250\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6481 - accuracy: 0.4122 - val_loss: 1.5859 - val_accuracy: 0.4376\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.6308 - accuracy: 0.4194 - val_loss: 1.5774 - val_accuracy: 0.4356\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.6356 - accuracy: 0.4189 - val_loss: 1.5745 - val_accuracy: 0.4396\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.6151 - accuracy: 0.4269 - val_loss: 1.5607 - val_accuracy: 0.4464\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5902 - accuracy: 0.4351 - val_loss: 1.5317 - val_accuracy: 0.4540\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.5697 - accuracy: 0.4405 - val_loss: 1.5195 - val_accuracy: 0.4526\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.5515 - accuracy: 0.4501 - val_loss: 1.4993 - val_accuracy: 0.4666\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5318 - accuracy: 0.4562 - val_loss: 1.4954 - val_accuracy: 0.4724\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5165 - accuracy: 0.4642 - val_loss: 1.4795 - val_accuracy: 0.4770\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.5000 - accuracy: 0.4710 - val_loss: 1.4752 - val_accuracy: 0.4688\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.4841 - accuracy: 0.4750 - val_loss: 1.4598 - val_accuracy: 0.4834\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4717 - accuracy: 0.4804 - val_loss: 1.4604 - val_accuracy: 0.4800\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.4602 - accuracy: 0.4800 - val_loss: 1.4482 - val_accuracy: 0.4878\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4439 - accuracy: 0.4886 - val_loss: 1.4451 - val_accuracy: 0.4912\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4359 - accuracy: 0.4893 - val_loss: 1.4424 - val_accuracy: 0.4892\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4266 - accuracy: 0.4960 - val_loss: 1.4251 - val_accuracy: 0.4976\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4114 - accuracy: 0.4983 - val_loss: 1.4357 - val_accuracy: 0.4884\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.4100 - accuracy: 0.5000 - val_loss: 1.4248 - val_accuracy: 0.5012\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3928 - accuracy: 0.5072 - val_loss: 1.4293 - val_accuracy: 0.5030\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3791 - accuracy: 0.5107 - val_loss: 1.4147 - val_accuracy: 0.5082\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.3717 - accuracy: 0.5126 - val_loss: 1.4081 - val_accuracy: 0.5124\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.3616 - accuracy: 0.5169 - val_loss: 1.4106 - val_accuracy: 0.5034\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3739 - accuracy: 0.5108 - val_loss: 1.4349 - val_accuracy: 0.4988\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3582 - accuracy: 0.5156 - val_loss: 1.4223 - val_accuracy: 0.4990\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3485 - accuracy: 0.5221 - val_loss: 1.4179 - val_accuracy: 0.5074\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3398 - accuracy: 0.5239 - val_loss: 1.3996 - val_accuracy: 0.5092\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3306 - accuracy: 0.5274 - val_loss: 1.4041 - val_accuracy: 0.5090\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3187 - accuracy: 0.5327 - val_loss: 1.4000 - val_accuracy: 0.5070\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.3101 - accuracy: 0.5377 - val_loss: 1.4001 - val_accuracy: 0.5072\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2947 - accuracy: 0.5416 - val_loss: 1.3884 - val_accuracy: 0.5102\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2842 - accuracy: 0.5444 - val_loss: 1.3821 - val_accuracy: 0.5192\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2791 - accuracy: 0.5484 - val_loss: 1.3808 - val_accuracy: 0.5228\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2714 - accuracy: 0.5502 - val_loss: 1.3853 - val_accuracy: 0.5172\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2572 - accuracy: 0.5556 - val_loss: 1.4019 - val_accuracy: 0.5096\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2519 - accuracy: 0.5543 - val_loss: 1.3838 - val_accuracy: 0.5176\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2473 - accuracy: 0.5583 - val_loss: 1.3851 - val_accuracy: 0.5172\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2305 - accuracy: 0.5604 - val_loss: 1.3944 - val_accuracy: 0.5166\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2302 - accuracy: 0.5625 - val_loss: 1.3811 - val_accuracy: 0.5216\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2216 - accuracy: 0.5691 - val_loss: 1.3879 - val_accuracy: 0.5258\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2062 - accuracy: 0.5733 - val_loss: 1.3832 - val_accuracy: 0.5268\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2026 - accuracy: 0.5755 - val_loss: 1.3867 - val_accuracy: 0.5218\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1936 - accuracy: 0.5773 - val_loss: 1.3925 - val_accuracy: 0.5172\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2438 - accuracy: 0.5589 - val_loss: 1.4291 - val_accuracy: 0.5050\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2372 - accuracy: 0.5596 - val_loss: 1.3971 - val_accuracy: 0.5146\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2304 - accuracy: 0.5656 - val_loss: 1.3985 - val_accuracy: 0.5158\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2247 - accuracy: 0.5638 - val_loss: 1.3780 - val_accuracy: 0.5144\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2138 - accuracy: 0.5723 - val_loss: 1.3955 - val_accuracy: 0.5210\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2068 - accuracy: 0.5748 - val_loss: 1.3889 - val_accuracy: 0.5178\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1931 - accuracy: 0.5782 - val_loss: 1.3929 - val_accuracy: 0.5134\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1824 - accuracy: 0.5805 - val_loss: 1.3769 - val_accuracy: 0.5178\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1697 - accuracy: 0.5847 - val_loss: 1.3907 - val_accuracy: 0.5278\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1675 - accuracy: 0.5888 - val_loss: 1.3841 - val_accuracy: 0.5238\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1594 - accuracy: 0.5905 - val_loss: 1.3921 - val_accuracy: 0.5160\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1471 - accuracy: 0.5965 - val_loss: 1.3890 - val_accuracy: 0.5160\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1359 - accuracy: 0.5960 - val_loss: 1.3998 - val_accuracy: 0.5290\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1325 - accuracy: 0.5994 - val_loss: 1.3916 - val_accuracy: 0.5204\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1181 - accuracy: 0.6029 - val_loss: 1.3868 - val_accuracy: 0.5310\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1181 - accuracy: 0.6036 - val_loss: 1.3849 - val_accuracy: 0.5308\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1098 - accuracy: 0.6062 - val_loss: 1.3735 - val_accuracy: 0.5332\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0931 - accuracy: 0.6121 - val_loss: 1.3833 - val_accuracy: 0.5332\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0909 - accuracy: 0.6120 - val_loss: 1.4033 - val_accuracy: 0.5216\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0824 - accuracy: 0.6166 - val_loss: 1.4041 - val_accuracy: 0.5274\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2676 - accuracy: 0.5520 - val_loss: 1.4593 - val_accuracy: 0.4986\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2588 - accuracy: 0.5542 - val_loss: 1.4311 - val_accuracy: 0.5080\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2422 - accuracy: 0.5629 - val_loss: 1.4417 - val_accuracy: 0.4992\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2394 - accuracy: 0.5613 - val_loss: 1.4423 - val_accuracy: 0.5034\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2218 - accuracy: 0.5708 - val_loss: 1.3945 - val_accuracy: 0.5182\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2134 - accuracy: 0.5724 - val_loss: 1.3951 - val_accuracy: 0.5290\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1972 - accuracy: 0.5772 - val_loss: 1.4057 - val_accuracy: 0.5162\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1829 - accuracy: 0.5850 - val_loss: 1.4035 - val_accuracy: 0.5218\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1648 - accuracy: 0.5914 - val_loss: 1.3741 - val_accuracy: 0.5292\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1497 - accuracy: 0.5948 - val_loss: 1.3717 - val_accuracy: 0.5390\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1416 - accuracy: 0.5986 - val_loss: 1.3754 - val_accuracy: 0.5322\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1238 - accuracy: 0.6038 - val_loss: 1.3842 - val_accuracy: 0.5202\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1124 - accuracy: 0.6086 - val_loss: 1.4157 - val_accuracy: 0.5290\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1025 - accuracy: 0.6113 - val_loss: 1.3716 - val_accuracy: 0.5318\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0887 - accuracy: 0.6172 - val_loss: 1.3941 - val_accuracy: 0.5276\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0840 - accuracy: 0.6167 - val_loss: 1.3826 - val_accuracy: 0.5308\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0698 - accuracy: 0.6216 - val_loss: 1.3734 - val_accuracy: 0.5360\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0569 - accuracy: 0.6267 - val_loss: 1.3704 - val_accuracy: 0.5326\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0530 - accuracy: 0.6283 - val_loss: 1.3920 - val_accuracy: 0.5386\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0377 - accuracy: 0.6349 - val_loss: 1.4290 - val_accuracy: 0.5258\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1298 - accuracy: 0.6035 - val_loss: 1.4954 - val_accuracy: 0.4986\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1215 - accuracy: 0.6055 - val_loss: 1.4506 - val_accuracy: 0.5114\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1188 - accuracy: 0.6062 - val_loss: 1.4521 - val_accuracy: 0.5046\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1175 - accuracy: 0.6093 - val_loss: 1.4644 - val_accuracy: 0.5048\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1009 - accuracy: 0.6160 - val_loss: 1.4223 - val_accuracy: 0.5192\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0993 - accuracy: 0.6141 - val_loss: 1.4049 - val_accuracy: 0.5192\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0795 - accuracy: 0.6207 - val_loss: 1.4230 - val_accuracy: 0.5272\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0682 - accuracy: 0.6246 - val_loss: 1.4051 - val_accuracy: 0.5270\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0589 - accuracy: 0.6297 - val_loss: 1.4063 - val_accuracy: 0.5350\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0535 - accuracy: 0.6326 - val_loss: 1.3996 - val_accuracy: 0.5342\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0449 - accuracy: 0.6336 - val_loss: 1.3861 - val_accuracy: 0.5362\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0316 - accuracy: 0.6399 - val_loss: 1.4444 - val_accuracy: 0.5186\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0177 - accuracy: 0.6428 - val_loss: 1.4157 - val_accuracy: 0.5242\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0080 - accuracy: 0.6483 - val_loss: 1.3918 - val_accuracy: 0.5342\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9950 - accuracy: 0.6500 - val_loss: 1.4420 - val_accuracy: 0.5328\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9965 - accuracy: 0.6505 - val_loss: 1.4117 - val_accuracy: 0.5392\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9825 - accuracy: 0.6581 - val_loss: 1.4044 - val_accuracy: 0.5410\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9738 - accuracy: 0.6567 - val_loss: 1.4039 - val_accuracy: 0.5342\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9709 - accuracy: 0.6593 - val_loss: 1.4538 - val_accuracy: 0.5304\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9609 - accuracy: 0.6661 - val_loss: 1.4544 - val_accuracy: 0.5184\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1283 - accuracy: 0.6067 - val_loss: 1.5035 - val_accuracy: 0.4976\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1336 - accuracy: 0.6047 - val_loss: 1.5051 - val_accuracy: 0.4988\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1244 - accuracy: 0.6113 - val_loss: 1.4526 - val_accuracy: 0.5228\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1266 - accuracy: 0.6094 - val_loss: 1.4327 - val_accuracy: 0.5246\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1159 - accuracy: 0.6121 - val_loss: 1.4621 - val_accuracy: 0.5128\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.1046 - accuracy: 0.6156 - val_loss: 1.4440 - val_accuracy: 0.5120\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0917 - accuracy: 0.6200 - val_loss: 1.4753 - val_accuracy: 0.5222\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0839 - accuracy: 0.6221 - val_loss: 1.4225 - val_accuracy: 0.5360\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0694 - accuracy: 0.6286 - val_loss: 1.4292 - val_accuracy: 0.5260\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0589 - accuracy: 0.6306 - val_loss: 1.4059 - val_accuracy: 0.5310\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0508 - accuracy: 0.6328 - val_loss: 1.4223 - val_accuracy: 0.5220\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0399 - accuracy: 0.6380 - val_loss: 1.4420 - val_accuracy: 0.5200\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0232 - accuracy: 0.6438 - val_loss: 1.5204 - val_accuracy: 0.5050\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.0290 - accuracy: 0.6440 - val_loss: 1.4250 - val_accuracy: 0.5314\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0054 - accuracy: 0.6510 - val_loss: 1.4709 - val_accuracy: 0.5138\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.0068 - accuracy: 0.6489 - val_loss: 1.4271 - val_accuracy: 0.5246\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9912 - accuracy: 0.6554 - val_loss: 1.4662 - val_accuracy: 0.5198\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9828 - accuracy: 0.6591 - val_loss: 1.4385 - val_accuracy: 0.5238\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9748 - accuracy: 0.6590 - val_loss: 1.4538 - val_accuracy: 0.5326\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 0.9658 - accuracy: 0.6655 - val_loss: 1.4907 - val_accuracy: 0.5154\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3355 - accuracy: 0.5441 - val_loss: 1.5778 - val_accuracy: 0.4854\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3386 - accuracy: 0.5373 - val_loss: 1.5208 - val_accuracy: 0.4838\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3130 - accuracy: 0.5490 - val_loss: 1.5916 - val_accuracy: 0.4672\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3064 - accuracy: 0.5496 - val_loss: 1.4926 - val_accuracy: 0.4946\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2744 - accuracy: 0.5583 - val_loss: 1.4718 - val_accuracy: 0.4972\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2584 - accuracy: 0.5623 - val_loss: 1.4688 - val_accuracy: 0.4958\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.2392 - accuracy: 0.5694 - val_loss: 1.4766 - val_accuracy: 0.5042\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.2178 - accuracy: 0.5791 - val_loss: 1.4430 - val_accuracy: 0.5128\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1998 - accuracy: 0.5860 - val_loss: 1.4457 - val_accuracy: 0.5024\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1755 - accuracy: 0.5947 - val_loss: 1.4513 - val_accuracy: 0.5224\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1646 - accuracy: 0.5956 - val_loss: 1.4158 - val_accuracy: 0.5238\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1467 - accuracy: 0.6056 - val_loss: 1.4255 - val_accuracy: 0.5214\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1306 - accuracy: 0.6096 - val_loss: 1.4585 - val_accuracy: 0.5172\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1138 - accuracy: 0.6162 - val_loss: 1.4461 - val_accuracy: 0.5108\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.1011 - accuracy: 0.6214 - val_loss: 1.4588 - val_accuracy: 0.5166\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0919 - accuracy: 0.6242 - val_loss: 1.4952 - val_accuracy: 0.5110\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0762 - accuracy: 0.6271 - val_loss: 1.4105 - val_accuracy: 0.5334\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0600 - accuracy: 0.6335 - val_loss: 1.3983 - val_accuracy: 0.5246\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0514 - accuracy: 0.6343 - val_loss: 1.5032 - val_accuracy: 0.5030\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0418 - accuracy: 0.6416 - val_loss: 1.4466 - val_accuracy: 0.5170\n",
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1569 - accuracy: 0.6002 - val_loss: 1.6262 - val_accuracy: 0.4774\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1564 - accuracy: 0.6002 - val_loss: 1.4466 - val_accuracy: 0.5164\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1400 - accuracy: 0.6060 - val_loss: 1.4752 - val_accuracy: 0.5142\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1357 - accuracy: 0.6083 - val_loss: 1.5223 - val_accuracy: 0.5128\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1142 - accuracy: 0.6137 - val_loss: 1.4472 - val_accuracy: 0.5132\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.1003 - accuracy: 0.6203 - val_loss: 1.4419 - val_accuracy: 0.5262\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0895 - accuracy: 0.6213 - val_loss: 1.5578 - val_accuracy: 0.5026\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0730 - accuracy: 0.6282 - val_loss: 1.4536 - val_accuracy: 0.5230\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0642 - accuracy: 0.6318 - val_loss: 1.4330 - val_accuracy: 0.5224\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.0548 - accuracy: 0.6344 - val_loss: 1.4718 - val_accuracy: 0.5230\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0454 - accuracy: 0.6397 - val_loss: 1.5092 - val_accuracy: 0.5152\n",
      "Epoch 12/20\n",
      "1407/1407 [==============================] - 15s 10ms/step - loss: 1.0326 - accuracy: 0.6427 - val_loss: 1.4597 - val_accuracy: 0.5188\n",
      "Epoch 13/20\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0165 - accuracy: 0.6498 - val_loss: 1.5651 - val_accuracy: 0.5152\n",
      "Epoch 14/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0123 - accuracy: 0.6489 - val_loss: 1.4261 - val_accuracy: 0.5222\n",
      "Epoch 15/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9981 - accuracy: 0.6554 - val_loss: 1.4681 - val_accuracy: 0.5266\n",
      "Epoch 16/20\n",
      "1407/1407 [==============================] - 18s 12ms/step - loss: 0.9974 - accuracy: 0.6532 - val_loss: 1.6478 - val_accuracy: 0.5192\n",
      "Epoch 17/20\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 0.9824 - accuracy: 0.6597 - val_loss: 81.3412 - val_accuracy: 0.5242\n",
      "Epoch 18/20\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 0.9743 - accuracy: 0.6614 - val_loss: 3.6679 - val_accuracy: 0.5310\n",
      "Epoch 19/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9641 - accuracy: 0.6652 - val_loss: 1.4932 - val_accuracy: 0.5216\n",
      "Epoch 20/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.9546 - accuracy: 0.6696 - val_loss: 1.5090 - val_accuracy: 0.5192\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define BN after activation model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# pick a good learning rate with 20 epochs\n",
    "lr_dict = defaultdict(list)\n",
    "lr_dict['lr'] = [1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3, 3e-3, 5e-3]\n",
    "\n",
    "for lr in lr_dict['lr']:\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=lr)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer = optimizer,\n",
    "                 metrics = ['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs = 20,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "    lr_dict['af_val_loss'].append(model.history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BN before activation model is not run due to long waiting time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing learning rate = 1e-05\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-bf7c24a5f047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                  metrics = ['accuracy'])\n\u001b[1;32m     22\u001b[0m     model.fit(X_train, y_train, epochs = 20,\n\u001b[0;32m---> 23\u001b[0;31m                 validation_data=(X_valid, y_valid))\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mlr_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bf_val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m                 ))\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    798\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1209\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1210\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1211\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m   \u001b[0;31m# TODO(b/151224785): Remove deprecated alias.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2583\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2943\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2945\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;31m# such as loss scaling and gradient clipping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n\u001b[0;32m--> 757\u001b[0;31m               self.trainable_variables)\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_minimize\u001b[0;34m(strategy, tape, optimizer, loss, trainable_variables)\u001b[0m\n\u001b[1;32m   2720\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scaled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m   \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m   \u001b[0;31m# Whether to aggregate gradients outside of optimizer. This requires support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1689\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5640\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m   5641\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5642\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   5643\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5644\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/stats/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    639\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m       \u001b[0mattr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default_value\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mattr_protos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define BN before activation model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, kernel_initializer='he_normal', use_bias=False))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# pick a good learning rate with 20 epochs\n",
    "for lr in lr_dict['lr']:\n",
    "    print('\\n\\nProcessing learning rate = {}'.format(lr))\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=lr)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer = optimizer,\n",
    "                 metrics = ['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs = 20,\n",
    "                validation_data=(X_valid, y_valid))\n",
    "    lr_dict['bf_val_loss'].append(model.history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BN after activation')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_lr = lr_dict['lr'][np.argmin(lr_dict['af_val_loss'])]\n",
    "lowest_loss = np.min(lr_dict['af_val_loss'])\n",
    "\n",
    "# pd.DataFrame(lr_dict).set_index('lr').plot()\n",
    "plt.plot(lr_dict['lr'], lr_dict['af_val_loss'], label=f'best lr = {best_lr:.0e} with loss = {lowest_loss:.2f}')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('BN after activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun model for 100 epochs with best_lr found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 1:09:43 - loss: 3.3255 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 5.9429s). Check your callbacks.\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 2.2277 - accuracy: 0.2154 - val_loss: 1.8954 - val_accuracy: 0.3078\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 15s 11ms/step - loss: 1.9041 - accuracy: 0.3150 - val_loss: 1.7629 - val_accuracy: 0.3598\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.8093 - accuracy: 0.3517 - val_loss: 1.6824 - val_accuracy: 0.3976\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.7389 - accuracy: 0.3776 - val_loss: 1.6357 - val_accuracy: 0.4178\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.6905 - accuracy: 0.3956 - val_loss: 1.5892 - val_accuracy: 0.4344\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.6502 - accuracy: 0.4112 - val_loss: 1.5540 - val_accuracy: 0.4428\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6127 - accuracy: 0.4261 - val_loss: 1.5371 - val_accuracy: 0.4498\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.5825 - accuracy: 0.4376 - val_loss: 1.5159 - val_accuracy: 0.4570\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.5623 - accuracy: 0.4458 - val_loss: 1.4991 - val_accuracy: 0.4650\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.5374 - accuracy: 0.4535 - val_loss: 1.4877 - val_accuracy: 0.4692\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.5187 - accuracy: 0.4619 - val_loss: 1.4727 - val_accuracy: 0.4650\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4945 - accuracy: 0.4684 - val_loss: 1.4569 - val_accuracy: 0.4786\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4745 - accuracy: 0.4738 - val_loss: 1.4534 - val_accuracy: 0.4788\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4588 - accuracy: 0.4840 - val_loss: 1.4403 - val_accuracy: 0.4858\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4394 - accuracy: 0.4876 - val_loss: 1.4456 - val_accuracy: 0.4914\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.4317 - accuracy: 0.4898 - val_loss: 1.4163 - val_accuracy: 0.4992\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4109 - accuracy: 0.4996 - val_loss: 1.4086 - val_accuracy: 0.4996\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3987 - accuracy: 0.5044 - val_loss: 1.4100 - val_accuracy: 0.4966\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3786 - accuracy: 0.5104 - val_loss: 1.4065 - val_accuracy: 0.5040\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3676 - accuracy: 0.5195 - val_loss: 1.4019 - val_accuracy: 0.5010\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3551 - accuracy: 0.5179 - val_loss: 1.3845 - val_accuracy: 0.5064\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3400 - accuracy: 0.5251 - val_loss: 1.3952 - val_accuracy: 0.5052\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3311 - accuracy: 0.5272 - val_loss: 1.3905 - val_accuracy: 0.5068\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3170 - accuracy: 0.5330 - val_loss: 1.3769 - val_accuracy: 0.5080\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3108 - accuracy: 0.5338 - val_loss: 1.3835 - val_accuracy: 0.5124\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.3026 - accuracy: 0.5370 - val_loss: 1.3767 - val_accuracy: 0.5048\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2897 - accuracy: 0.5401 - val_loss: 1.3752 - val_accuracy: 0.5104\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.2848 - accuracy: 0.5464 - val_loss: 1.3857 - val_accuracy: 0.5036\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2702 - accuracy: 0.5504 - val_loss: 1.3757 - val_accuracy: 0.5126\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2622 - accuracy: 0.5490 - val_loss: 1.3691 - val_accuracy: 0.5128\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2505 - accuracy: 0.5576 - val_loss: 1.3643 - val_accuracy: 0.5202\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2457 - accuracy: 0.5584 - val_loss: 1.3696 - val_accuracy: 0.5152\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2339 - accuracy: 0.5644 - val_loss: 1.3698 - val_accuracy: 0.5162\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2271 - accuracy: 0.5644 - val_loss: 1.3592 - val_accuracy: 0.5214\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2198 - accuracy: 0.5646 - val_loss: 1.3654 - val_accuracy: 0.5240\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2160 - accuracy: 0.5663 - val_loss: 1.3700 - val_accuracy: 0.5248\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2008 - accuracy: 0.5747 - val_loss: 1.3772 - val_accuracy: 0.5172\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1962 - accuracy: 0.5746 - val_loss: 1.3720 - val_accuracy: 0.5148\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1887 - accuracy: 0.5762 - val_loss: 1.3708 - val_accuracy: 0.5172\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1854 - accuracy: 0.5802 - val_loss: 1.3722 - val_accuracy: 0.5174\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1770 - accuracy: 0.5810 - val_loss: 1.3651 - val_accuracy: 0.5218\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1707 - accuracy: 0.5836 - val_loss: 1.3797 - val_accuracy: 0.5182\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1607 - accuracy: 0.5893 - val_loss: 1.3834 - val_accuracy: 0.5136\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.1575 - accuracy: 0.5898 - val_loss: 1.3800 - val_accuracy: 0.5218\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.3592 - accuracy: 0.5214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3592209815979004, 0.521399974822998]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define BN after activation model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, activation='elu', kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# let optimizer use best lr found\n",
    "best_lr = lr_dict['lr'][np.argmin(lr_dict['af_val_loss'])]\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=best_lr)\n",
    "\n",
    "# define callbacks\n",
    "run_index = 3\n",
    "run_logdir = os.path.join(os.curdir, 'my_cifar10_logs', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# checkpoint_cb = keras.callbacks.ModelCheckpoint('my_models/cifar10.h5', save_best_only=True)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer = optimizer,\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, epochs = 100,\n",
    "         validation_data=(X_valid, y_valid), \n",
    "         callbacks=[earlystop_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.3592 - accuracy: 0.5214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3592209815979004, 0.521399974822998]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. SELU\n",
    "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 2:51 - loss: 3.0071 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0070s vs `on_train_batch_end` time: 0.2371s). Check your callbacks.\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.9002 - accuracy: 0.3223 - val_loss: 1.8407 - val_accuracy: 0.3538\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6886 - accuracy: 0.4015 - val_loss: 1.7004 - val_accuracy: 0.3894\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5869 - accuracy: 0.4402 - val_loss: 1.6567 - val_accuracy: 0.4100\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5144 - accuracy: 0.4642 - val_loss: 1.6198 - val_accuracy: 0.4412\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4567 - accuracy: 0.4884 - val_loss: 1.5307 - val_accuracy: 0.4576\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.4099 - accuracy: 0.5030 - val_loss: 1.5200 - val_accuracy: 0.4714\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.3615 - accuracy: 0.5257 - val_loss: 1.5628 - val_accuracy: 0.4542\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3226 - accuracy: 0.5409 - val_loss: 1.4870 - val_accuracy: 0.4824\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2830 - accuracy: 0.5546 - val_loss: 1.5002 - val_accuracy: 0.4822\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2485 - accuracy: 0.5663 - val_loss: 1.5006 - val_accuracy: 0.4896\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 1.2129 - accuracy: 0.5799 - val_loss: 1.5473 - val_accuracy: 0.4910\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1803 - accuracy: 0.5937 - val_loss: 1.4789 - val_accuracy: 0.5030\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1533 - accuracy: 0.6036 - val_loss: 1.4999 - val_accuracy: 0.5010\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.1295 - accuracy: 0.6122 - val_loss: 1.4597 - val_accuracy: 0.5042\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0959 - accuracy: 0.6237 - val_loss: 1.4826 - val_accuracy: 0.5094\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 1.0777 - accuracy: 0.6326 - val_loss: 1.5401 - val_accuracy: 0.5074\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0501 - accuracy: 0.6411 - val_loss: 1.5395 - val_accuracy: 0.4958\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0297 - accuracy: 0.6508 - val_loss: 1.4981 - val_accuracy: 0.5088\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0119 - accuracy: 0.6558 - val_loss: 1.5693 - val_accuracy: 0.5020\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9886 - accuracy: 0.6633 - val_loss: 1.5627 - val_accuracy: 0.5034\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9654 - accuracy: 0.6698 - val_loss: 1.5864 - val_accuracy: 0.5050\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9484 - accuracy: 0.6788 - val_loss: 1.5475 - val_accuracy: 0.5030\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9267 - accuracy: 0.6837 - val_loss: 1.6071 - val_accuracy: 0.5044\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9070 - accuracy: 0.6920 - val_loss: 1.6398 - val_accuracy: 0.5006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19030f250>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# standardize the input\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_mean) / X_std\n",
    "X_valid_scaled = (X_valid - X_mean) / X_std\n",
    "X_test_scaled = (X_test - X_mean) / X_std\n",
    "\n",
    "# define dense model with selu as activation\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train_scaled.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# model has been changed, so the previous best lr may not be applicable here any more.\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "\n",
    "# define callbacks\n",
    "run_index = 4\n",
    "run_logdir = os.path.join(os.curdir, 'my_cifar10_logs', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# checkpoint_cb = keras.callbacks.ModelCheckpoint('my_models/cifar10.h5', save_best_only=True)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer = optimizer,\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(X_train_scaled, y_train, epochs = 100,\n",
    "         validation_data=(X_valid_scaled, y_valid), \n",
    "         callbacks=[earlystop_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4597 - accuracy: 0.5042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4596701860427856, 0.5041999816894531]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. SELU with MC alpha dropout\n",
    "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   2/1407 [..............................] - ETA: 2:44 - loss: 2.9857 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_train_batch_end` time: 0.2268s). Check your callbacks.\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.8973 - accuracy: 0.3245 - val_loss: 1.7629 - val_accuracy: 0.3738\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.6699 - accuracy: 0.4083 - val_loss: 1.6686 - val_accuracy: 0.4106\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.5760 - accuracy: 0.4470 - val_loss: 1.6626 - val_accuracy: 0.4274\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5062 - accuracy: 0.4709 - val_loss: 1.5549 - val_accuracy: 0.4532\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4490 - accuracy: 0.4949 - val_loss: 1.7011 - val_accuracy: 0.4476\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.4065 - accuracy: 0.5115 - val_loss: 1.5401 - val_accuracy: 0.4778\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3609 - accuracy: 0.5255 - val_loss: 1.6347 - val_accuracy: 0.4606\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.3199 - accuracy: 0.5407 - val_loss: 1.5209 - val_accuracy: 0.4836\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2842 - accuracy: 0.5556 - val_loss: 1.5209 - val_accuracy: 0.4812\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2472 - accuracy: 0.5696 - val_loss: 1.5358 - val_accuracy: 0.4894\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2194 - accuracy: 0.5807 - val_loss: 1.6186 - val_accuracy: 0.4856\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1906 - accuracy: 0.5898 - val_loss: 1.5054 - val_accuracy: 0.4968\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1611 - accuracy: 0.5992 - val_loss: 1.5723 - val_accuracy: 0.5040\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1359 - accuracy: 0.6087 - val_loss: 1.5577 - val_accuracy: 0.5128\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1091 - accuracy: 0.6194 - val_loss: 1.5443 - val_accuracy: 0.5036\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0860 - accuracy: 0.6303 - val_loss: 1.6342 - val_accuracy: 0.5082\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0586 - accuracy: 0.6377 - val_loss: 1.6089 - val_accuracy: 0.5074\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0364 - accuracy: 0.6451 - val_loss: 1.6877 - val_accuracy: 0.5022\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0829 - accuracy: 0.6335 - val_loss: 1.6090 - val_accuracy: 0.5196\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9865 - accuracy: 0.6633 - val_loss: 1.6794 - val_accuracy: 0.5142\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9630 - accuracy: 0.6731 - val_loss: 1.6936 - val_accuracy: 0.5142\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 11s 7ms/step - loss: 0.9511 - accuracy: 0.6751 - val_loss: 1.7241 - val_accuracy: 0.5104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199013110>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# standardize the input\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_mean) / X_std\n",
    "X_valid_scaled = (X_valid - X_mean) / X_std\n",
    "X_test_scaled = (X_test - X_mean) / X_std\n",
    "\n",
    "# define dense model with selu as activation\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train_scaled.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# model has been changed, so the previous best lr may not be applicable here any more.\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "\n",
    "# define callbacks\n",
    "run_index = 5\n",
    "run_logdir = os.path.join(os.curdir, 'my_cifar10_logs', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "# checkpoint_cb = keras.callbacks.ModelCheckpoint('my_models/cifar10.h5', save_best_only=True)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer = optimizer,\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(X_train_scaled, y_train, epochs = 100,\n",
    "         validation_data=(X_valid_scaled, y_valid), \n",
    "         callbacks=[earlystop_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5054 - accuracy: 0.4968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.505418062210083, 0.4968000054359436]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement MCDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass alphadropout\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "# create a new model replacing alphadropout with MCalphadropout\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some utils functions\n",
    "def predict_mc_probas(mc_model, inputs, runs=10):\n",
    "    probas = [mc_model.predict(inputs) for _ in range(runs)]\n",
    "    mean_probas = np.mean(probas, axis=0)\n",
    "    return mean_probas\n",
    "\n",
    "def predict_mc_class(mc_model, inputs, runs=10):\n",
    "    mean_probas = predict_mc_probas(mc_model, inputs, runs)\n",
    "    return np.argmax(mean_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4966"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_valid_pred = predict_mc_class(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_valid.ravel() == y_valid_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. 1cycle\n",
    "*Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define dense model with selu as activation\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train_scaled.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 4s 3ms/step - loss: nan - accuracy: 0.1542\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19829562"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates[np.argmin(losses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr0 = 0.015\n",
    "max_lr = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# define dense model with selu as activation\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train_scaled.shape[1:]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(units=100, activation='selu', kernel_initializer='lecun_normal'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.9011 - accuracy: 0.3226 - val_loss: 1.8523 - val_accuracy: 0.3310\n",
      "Epoch 2/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.6931 - accuracy: 0.3969 - val_loss: 2.1477 - val_accuracy: 0.3182\n",
      "Epoch 3/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.6369 - accuracy: 0.4211 - val_loss: 2.0074 - val_accuracy: 0.3690\n",
      "Epoch 4/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.6001 - accuracy: 0.4354 - val_loss: 2.0351 - val_accuracy: 0.3612\n",
      "Epoch 5/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5805 - accuracy: 0.4489 - val_loss: 1.9343 - val_accuracy: 0.3644\n",
      "Epoch 6/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5753 - accuracy: 0.4547 - val_loss: 1.6658 - val_accuracy: 0.4134\n",
      "Epoch 7/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5672 - accuracy: 0.4552 - val_loss: 1.8454 - val_accuracy: 0.3796\n",
      "Epoch 8/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5651 - accuracy: 0.4605 - val_loss: 1.8423 - val_accuracy: 0.3846\n",
      "Epoch 9/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5754 - accuracy: 0.4561 - val_loss: 1.8649 - val_accuracy: 0.4168\n",
      "Epoch 10/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5791 - accuracy: 0.4614 - val_loss: 2.0468 - val_accuracy: 0.3614\n",
      "Epoch 11/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5990 - accuracy: 0.4549 - val_loss: 2.2703 - val_accuracy: 0.3750\n",
      "Epoch 12/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.6014 - accuracy: 0.4520 - val_loss: 1.9387 - val_accuracy: 0.4028\n",
      "Epoch 13/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5402 - accuracy: 0.4747 - val_loss: 1.9134 - val_accuracy: 0.4060\n",
      "Epoch 14/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.4737 - accuracy: 0.4973 - val_loss: 1.8012 - val_accuracy: 0.4038\n",
      "Epoch 15/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.4183 - accuracy: 0.5201 - val_loss: 1.7453 - val_accuracy: 0.4118\n",
      "Epoch 16/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.3526 - accuracy: 0.5404 - val_loss: 1.6570 - val_accuracy: 0.4872\n",
      "Epoch 17/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2904 - accuracy: 0.5624 - val_loss: 1.6655 - val_accuracy: 0.4820\n",
      "Epoch 18/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2370 - accuracy: 0.5794 - val_loss: 1.5134 - val_accuracy: 0.5028\n",
      "Epoch 19/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.1737 - accuracy: 0.6015 - val_loss: 1.6474 - val_accuracy: 0.5072\n",
      "Epoch 20/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.1153 - accuracy: 0.6218 - val_loss: 1.5128 - val_accuracy: 0.5226\n",
      "Epoch 21/25\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.0563 - accuracy: 0.6413 - val_loss: 1.5509 - val_accuracy: 0.5200\n",
      "Epoch 22/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.9991 - accuracy: 0.6608 - val_loss: 1.5551 - val_accuracy: 0.5240\n",
      "Epoch 23/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.9397 - accuracy: 0.6809 - val_loss: 1.5846 - val_accuracy: 0.5226\n",
      "Epoch 24/25\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.8993 - accuracy: 0.6972 - val_loss: 1.6068 - val_accuracy: 0.5262\n",
      "Epoch 25/25\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.8672 - accuracy: 0.7090 - val_loss: 1.6311 - val_accuracy: 0.5278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196757cd0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=max_lr)\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs = n_epochs, batch_size=batch_size,\n",
    "         validation_data=(X_valid_scaled, y_valid), \n",
    "         callbacks=[onecycle])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
